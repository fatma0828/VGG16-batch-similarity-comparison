# -*- coding: utf-8 -*-
"""
Created on Wed Oct 18 20:14:55 2023

@author: matt
@specialthanksto: Yuuma
"""
from multiprocessing import set_start_method
import multiprocessing
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.models import Model
import numpy as np
import pandas as pd
import logging
from os import getpid
import warnings
import time


# Logger for each processor
def setup_process_logger():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s')


# Main extract function using the VGG16 model
def extract(img, layer_name, base_model):
    # Load image
    img = load_img(img)
    # Resize the image
    img = img.resize((224, 224))
    # Convert the image color space
    img = img.convert('RGB')
    # Reformat the image
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    # Extract Features
    model = Model(inputs=base_model.input, outputs=base_model.get_layer(layer_name).output)
    feature = model.predict(x)[0]
    return feature / np.linalg.norm(feature)


# Main thread function for each processor
def thread_function(in_queue, out_queue):
    setup_process_logger()

    base_model = VGG16(weights='imagenet')

    layernames = ['block1_conv1', 'block1_pool', 'block2_pool', 'block3_conv3', 'block3_pool', 'block4_conv1',
                  'block4_pool', 'block5_pool', 'flatten', 'fc1', 'fc2']

    while True:
        in_arr = in_queue.get()
        if in_arr is None:
            break

        try:
            img1, img2 = in_arr
            # Set up dict
            feature_values1 = {}
            feature_values2 = {}
            for layers in layernames:
                img_feature1 = extract(img1, layers, base_model)
                feature_values1[layers] = img_feature1

                img_feature2 = extract(img2, layers, base_model)
                feature_values2[layers] = img_feature2

            # Calculate distances
            euclidean_distances = {}
            distance_dict = {}

            for layer in feature_values1:
                distance_dict[layer] = np.linalg.norm(feature_values1[layer] - feature_values2[layer])

            img1name = os.path.basename(img1)
            img2name = os.path.basename(img2)
            euclidean_distances[(img1name, img2name)] = distance_dict

            logging.info("Process {} processed {} and {}.".format(getpid(), img1name, img2name))

            out_queue.put(euclidean_distances)

        except Exception as e:
            logging.error("An error occurred in thread_function: %s", str(e))


if __name__ == '__main__':

    # Filter all warning messages
    warnings.filterwarnings("ignore")
    tf.get_logger().setLevel('FATAL')
    # Manual fix for multiprocessing deadlock bug in old Python versions. This is not needed in versions where the fork() issue is fixed.
    set_start_method("spawn")

    # IMGPATH MAKER start
    directory = os.getcwd()  # Specify the directory path HERE

    imgpoolpaths = []

    for root, dirs, files in os.walk(directory):
        jpg_files = [file for file in files if file.lower().endswith(".jpg")]
        for i, file in enumerate(jpg_files):
            if i >= 3:
                break
            f = os.path.join(root, file)
            # checking if it is a file
            if os.path.isfile(f):
                imgpoolpaths.append(f)

    # Set the name of the CSV file
    input_csv_file = "results.csv"

    # Start the timer
    start_time = time.time()

    # Check if the CSV file exists and load data from the file
    if os.path.isfile(input_csv_file):
        df = pd.read_csv(input_csv_file)

        # Set the first two columns as a tuple index
        df.set_index(['Unnamed: 0', 'Unnamed: 1'], inplace=True)

        # Convert the DataFrame to a dictionary
        finalresults = df.to_dict(orient='index')

    else:
        proceed = input(f"Warning: The CSV file '{input_csv_file}' does not exist. Proceed? Y/N")

        if proceed != "Y":
            raise SystemExit

        finalresults = {}

    image_names = [os.path.basename(file) for file in imgpoolpaths]

    num_images = len(imgpoolpaths)
    print(f"Number of images: {num_images}")

    # IMGPATH MAKER end

    # MISSION LIST start

    missionlist = []

    # Iterate through the list of images and find duplicates
    for i in range(num_images):

        img1 = image_names[i]

        for j in range(i + 1):

            img2 = image_names[j]

            if img1 == img2:
                continue

            # Check if the tuple already exists in finalresults
            if (img1, img2) in finalresults or (img2, img1) in finalresults:
                print(f"Skipping {img1} and {img2}.")
                continue

            missionlist.append((imgpoolpaths[i], imgpoolpaths[j]))

    # For debugging: print(missionlist)

    # MISSION LIST end

    # Main worker distribution code
    num_workers = 25    # Set number of workers; be careful of system load
    write_threshold = 100    # Will write to file per X lines

    in_queue_list = [multiprocessing.Queue() for i in range(num_workers)]
    out_queue = multiprocessing.Queue()

    p_list = [multiprocessing.Process(target=thread_function, args=(in_queue_list[i], out_queue)) for i in range(num_workers)]
    [p.start() for p in p_list]

    time0 = time.time()    # For evaluation time usage only

    # Distributing tasks
    for i, mission in enumerate(missionlist):
        in_queue_list[i % num_workers].put(mission)

    # Adding end signal to all workers in the end
    [in_queue_list[i].put(None) for i in range(num_workers)]

    # Fetching results from queue
    out_list = []

    for i in range(len(missionlist)):
        out_list.append(out_queue.get())
        print("out_list appended")

        # Write to file per X lines
        if len(out_list) >= write_threshold:
            print("write threshold triggered")
            # Write results to file
            for result in out_list:
                # Create a DataFrame from the calculated distances
                df = pd.DataFrame(result).T

                # Fill NaN values with an empty dictionary
                df.fillna({}, inplace=True)

                if os.path.isfile(input_csv_file):
                    df.to_csv(input_csv_file, mode='a', header=False)
                else:
                    df.to_csv(input_csv_file)
            # Clear out_arr
            out_list = []

    # Write remaining lines to file
    if len(out_list):
        print(f"Writing last {len(out_list)} entries.")
        # Write results to file
        for result in out_list:
            # Create a DataFrame from the calculated distances
            df = pd.DataFrame(result).T

            # Fill NaN values with an empty dictionary
            df.fillna({}, inplace=True)

            if os.path.isfile(input_csv_file):
                df.to_csv(input_csv_file, mode='a', header=False)
            else:
                df.to_csv(input_csv_file)

    # End multiprocessing pool
    print("p join starting")
    [p.join() for p in p_list]

    # Calculate the elapsed time
    elapsed_time = time.time() - time0
    print(f"The process took {elapsed_time} seconds.")
